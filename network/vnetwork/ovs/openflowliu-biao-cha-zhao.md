# OpenFlow

除了基于overlay网络的思想设计以外，OpenVSwitch的另一大特点就是基于OpenFlow。

传统的交换机，不论是硬件的，还是软件的，所具备的功能都是预先内置的，需要使用某个功能的时候，进行相应的配置即可。而OpenVSwitch通过OpenFlow实现了交换机的可编程。OpenFlow可以定义网络包在交换机中的处理流程（pipeline），因此支持OpenFlow的交换机，其功能不再是固定的，通过OpenFlow可以软件定义OpenVSwitch所具备的功能。

OpenFlow以多个Table串行工作的方式来处理网络数据包，如下图所示。

OpenFlow的灵活性是实现SDN必不可少的一部分，但是在一些实际场景中，因为涉及的功能多且复杂，相应的OpenFlow pipeline会变得很长。直观上来看，pipeline越长，处理一个网络包需要的时间也越长。这是OpenFlow从理论到实际的一个问题，OpenVSwitch为此尝试过很多优化。

对于一个Linux系统来说，可以分为用户空间（user space）和内核空间（kernel space），网络设备接入到内核空间。如果需要将数据传输到用户程序则需要通过内核空间将数据上送到用户空间，如果需要在网络设备之间转发数据，直接在内核空间就可以完成。

作为运行在x86服务器中的软件交换机，直观上来看，应该在内核空间来实现转发。因此，OpenVSwitch在最早期的时候，在Linux内核模块实现了所有的OpenFlow的处理。当时的OpenVSwitch内核模块，接收网络数据包，根据OpenFlow规则，一步步的Match，并根据Action修改网络数据包，最后从某个网络设备送出。

但是这种方式很快就被认为是不能实际应用的。首先，虽然在内核实现可以缩短网络数据包在操作系统的路径，但是在内核进行程序开发和更新也更加困难，以OpenVSwitch的更新速度，完全在内核实现将变得不切实际。其次，完全按照OpenFlow pipeline去处理网络包，势必要消耗大量CPU，进而降低网络性能。

因此，最新版本（2.x版本）的OpenVSwitch采用了一种很不一样的方式来避免这些问题。





