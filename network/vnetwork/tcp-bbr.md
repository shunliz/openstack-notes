# TCP BBR {#h1-tcp-bbr}

TCP BBR 已经在 Youtube 服务器和 Google 跨数据中心的内部广域网（B4）上部署。

* 论文：[http://queue.acm.org/detail.cfm?id=3022184](http://queue.acm.org/detail.cfm?id=3022184)
* Linux内核：4.9+[commit](http://git.kernel.org/cgit/linux/kernel/git/davem/net-next.git/commit/?id=0f8782ea14974ce992618b55f0c041ef43ed0b78)



* 即时速率的计算：该带宽是bbr一切计算的基准，bw=应答数据/应答这些数据所用时间（只关注数据的大小，不关注数据的含义） 
  ![](/assets/network-virtualnet-linuxnet-tcp-bbr1.png)
* RTT跟踪：系统会跟踪当前为止最小RTT
* bbr pipe状态机：STARTUP，DRAIN，PROBE\_BW，PROBE\_RTT
  ![](/assets/network-virtualnet-linuxnet-tcp-bbr2.png)
  ![](/assets/network-virtualnet-linuxnet-tcp-bbr3.png)
  * bbr处在各个状态时的增益系数：
    * 结果输出：pacing rate（规定cwnd指示的一窗数据的数据包之间，以多大的时间间隔发送出去）和cwnd
      * pacing rate怎么计算？很简单，就是是使用时间窗口内\(默认10轮采样\)最大BW。上一次采样的即时BW，用它来在可能的情况下更新时间窗口内的BW采样值集合。这次能否按照这个时间窗口内最大BW发送数据呢？这样看当前的增益系数的值，设为G，那么BW\*G就是pacing rate的值，是不是很简单呢？！
        * 至于说cwnd的计算可能要稍微复杂一点，但是也是可以理解的，我们知道，cwnd其实描述了一条网络管道\(rwnd描述了接收端缓冲区\)，因此cwnd其实就是这个管道的容量，也就是BDP！
        * BW我们已经有了，缺少的是D，也就是RTT，不过别忘了，bbr一直在持续搜集最小的RTT值，注意，bbr并没有采用什么移动指数平均算法来“猜测”RTT\(我用猜测而不是预测的原因是，猜测的结果往往更加不可信！\)，而是直接冒泡采集最小的RTT\(注意这个RTT是TCP系统层面移动指数平均的结果，即SRTT，但brr并不会对此结果再次做平均！\)。我们用这个最小RTT干什么呢？ 当前是计算BDP了！这里bbr取的RTT就是这个最小RTT。最小RTT表示一个曾经达到的最佳RTT，既然曾经达到过，说明这是客观的可以再次达到的RTT，这样有益于网络管道利用率最大化！
        * 我们采用BDP\*G’就算出了cwnd，这里的G’是cwnd的增益系数，与带宽增益系数含义一样，根据bbr的状态机来获取！

    bbr确实忽略了Recovery等非Open的拥塞状态:![](/assets/network-virtualnet-linuxnet-tcp-bbr4.png)

## BBR算法 {#h2-bbr-}

**TCP BBR 致力于解决两个问题：**

1. **在有一定丢包率的网络链路上充分利用带宽。**
2. **降低网络链路上的 buffer 占用率，从而降低延迟。**

TCP 拥塞控制的目标是最大化利用网络上瓶颈链路的带宽: 网络内尚未被确认收到的数据包数量 = 网络链路上能容纳的数据包数量 = 链路带宽 × 往返延迟。

TCP 维护一个发送窗口，估计当前网络链路上能容纳的数据包数量，希望在有数据可发的情况下，回来一个确认包就发出一个数据包，总是保持发送窗口那么多个包在网络中流动。标准 TCP 中的拥塞控制算法也类似：不断增加发送窗口，直到发现开始丢包。这就是所谓的 ”加性增，乘性减”，也就是当收到一个确认消息的时候慢慢增加发送窗口，当确认一个包丢掉的时候较快地减小发送窗口。

标准 TCP 的这种做法有两个问题：

* 首先，假定网络中的丢包都是由于拥塞导致（网络设备的缓冲区放不下了，只好丢掉一些数据包）。事实上网络中有可能存在传输错误导致的丢包，基于丢包的拥塞控制算法并不能区分拥塞丢包和错误丢包。在数据中心内部，错误丢包率在十万分之一（1e-5）的量级；在广域网上，错误丢包率一般要高得多。更重要的是，“加性增，乘性减” 的拥塞控制算法要能正常工作，错误丢包率需要与发送窗口的平方成反比。数据中心内的延迟一般是 10-100 微秒，带宽 10-40 Gbps，乘起来得到稳定的发送窗口为 12.5 KB 到 500 KB。而广域网上的带宽可能是 100 Mbps，延迟 100 毫秒，乘起来得到稳定的发送窗口为 10 MB。广域网上的发送窗口比数据中心网络高 1-2 个数量级，错误丢包率就需要低 2-4 个数量级才能正常工作。因此标准 TCP 在有一定错误丢包率的长肥管道（long-fat pipe，即延迟高、带宽大的链路）上只会收敛到一个很小的发送窗口。这就是很多时候客户端和服务器都有很大带宽，运营商核心网络也没占满，但下载速度很慢，甚至下载到一半就没速度了的一个原因。
* 其次，网络中会有一些 buffer，就像输液管里中间膨大的部分，用于吸收网络中的流量波动。由于标准 TCP 是通过 “灌满水管” 的方式来估算发送窗口的，在连接的开始阶段，buffer 会被倾向于占满。后续 buffer 的占用会逐渐减少，但是并不会完全消失。客户端估计的水管容积（发送窗口大小）总是略大于水管中除去膨大部分的容积。这个问题被称为 bufferbloat（缓冲区膨胀）。

缓冲区膨胀有两个危害：

* 增加网络延迟。buffer 里面的东西越多，要等的时间就越长嘛。
* 共享网络瓶颈的连接较多时，可能导致缓冲区被填满而丢包。很多人把这种丢包认为是发生了网络拥塞，实则不然。



